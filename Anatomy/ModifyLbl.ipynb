{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ModifyLbl.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMYyxDbBYaJrjVzWAtJIRLw"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K5101ysQdOOt","executionInfo":{"status":"ok","timestamp":1614188188331,"user_tz":420,"elapsed":18966,"user":{"displayName":"Jaydeep Chakraborty","photoUrl":"","userId":"15937785283944377251"}},"outputId":"d02c12e8-efc0-45e6-8234-f48303f6c125"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","DIR='/content/drive/MyDrive/Research/OntoConnectWithGNN/GNN_1/'\n","\n","import os\n","os.chdir(DIR)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":100},"id":"Lyr5cEoh8LV2","executionInfo":{"status":"ok","timestamp":1614188254010,"user_tz":420,"elapsed":63638,"user":{"displayName":"Jaydeep Chakraborty","photoUrl":"","userId":"15937785283944377251"}},"outputId":"1e69bcd9-b965-4d17-84c9-e79450ee639d"},"source":["from IPython.display import Javascript  # Restrict height of output cell.\n","display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 100})'''))\n","\n","!python -c \"import torch; print(torch.__version__)\"\n","!python -c \"import torch; print(torch.version.cuda)\"\n","\n","!pip install git+https://github.com/facebookresearch/fastText.git\n","!pip install git+https://github.com/facebookresearch/PyTorch-BigGraph.git"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["google.colab.output.setIframeHeight(0, true, {maxHeight: 100})"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["1.7.0+cu101\n","10.1\n","Collecting git+https://github.com/facebookresearch/fastText.git\n","  Cloning https://github.com/facebookresearch/fastText.git to /tmp/pip-req-build-7neu76ee\n","  Running command git clone -q https://github.com/facebookresearch/fastText.git /tmp/pip-req-build-7neu76ee\n","Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (2.6.2)\n","Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (53.0.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (1.19.5)\n","Building wheels for collected packages: fasttext\n","  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3088324 sha256=91277e8737514a5b80bef0817d9b51d6f7a3b1cf9146d6f93459d65b973c7d3a\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-cohkv14d/wheels/69/f8/19/7f0ab407c078795bc9f86e1f6381349254f86fd7d229902355\n","Successfully built fasttext\n","Installing collected packages: fasttext\n","Successfully installed fasttext-0.9.2\n","Collecting git+https://github.com/facebookresearch/PyTorch-BigGraph.git\n","  Cloning https://github.com/facebookresearch/PyTorch-BigGraph.git to /tmp/pip-req-build-4ruh074x\n","  Running command git clone -q https://github.com/facebookresearch/PyTorch-BigGraph.git /tmp/pip-req-build-4ruh074x\n","Requirement already satisfied: attrs>=18.2 in /usr/local/lib/python3.7/dist-packages (from torchbiggraph==1.0.1.dev0) (20.3.0)\n","Requirement already satisfied: h5py>=2.8 in /usr/local/lib/python3.7/dist-packages (from torchbiggraph==1.0.1.dev0) (2.10.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchbiggraph==1.0.1.dev0) (1.19.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from torchbiggraph==1.0.1.dev0) (53.0.0)\n","Requirement already satisfied: torch>=1 in /usr/local/lib/python3.7/dist-packages (from torchbiggraph==1.0.1.dev0) (1.7.0+cu101)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchbiggraph==1.0.1.dev0) (4.41.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py>=2.8->torchbiggraph==1.0.1.dev0) (1.15.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1->torchbiggraph==1.0.1.dev0) (3.7.4.3)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch>=1->torchbiggraph==1.0.1.dev0) (0.16.0)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch>=1->torchbiggraph==1.0.1.dev0) (0.6)\n","Building wheels for collected packages: torchbiggraph\n","  Building wheel for torchbiggraph (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torchbiggraph: filename=torchbiggraph-1.0.1.dev0-cp37-none-any.whl size=118953 sha256=1dab5a30737e0feb2ea150218d1ec6f1e26a12ec03e914201f5eb13e5edc3b88\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-l9uszkxj/wheels/d5/d0/e8/9fa5ee999e79534d0c4cb2c9127e08cbf42ba9f1e701158a33\n","Successfully built torchbiggraph\n","Installing collected packages: torchbiggraph\n","Successfully installed torchbiggraph-1.0.1.dev0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NQq7o1AeiNUd","executionInfo":{"status":"ok","timestamp":1614188340430,"user_tz":420,"elapsed":3359,"user":{"displayName":"Jaydeep Chakraborty","photoUrl":"","userId":"15937785283944377251"}},"outputId":"fe429e2f-96bd-42c1-c84c-401e801b2632"},"source":["#from OntoSimImports import *\n","%run OntoSimImports.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"s6kz4r0FJshe"},"source":["#import OntoSimConstants\n","%run OntoSimConstants.py\n","from OntoSimConstants import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0tgbB7pJP-AK"},"source":["def assignVar():\n","    conf_1 = {\n","        'src_ip_fl_nm': DATA_DIR+'ip/source.json',\n","        'trgt_ip_fl_nm': DATA_DIR+'ip/target.json',\n","        'src_op_fl_nm': DATA_DIR+'modifylbl/source.json',\n","        'trgt_op_fl_nm': DATA_DIR+'modifylbl/target.json',\n","        'removed_utl_fl_nm': DATA_DIR+'util/removed.txt'\n","    }\n","\n","    conf_arr = []\n","    conf_arr.append(conf_1)\n","\n","    return conf_arr\n","\n","\n","conf_arr = assignVar()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1FaGkQyhTv8l"},"source":["def getDataParam():\n","    data_param_fl_nm = code_path + data_param_json\n","    with open(data_param_fl_nm) as f:\n","        data_param = json.load(f)\n","    return data_param"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yIfdhYx0c5eF"},"source":["def loadSourceTarget(conf):\n","    source_fl_nm = conf['src_ip_fl_nm']\n","    target_fl_nm = conf['trgt_ip_fl_nm']\n","\n","    source_fl_nm = code_path + source_fl_nm\n","    with open(source_fl_nm) as f:\n","        source_data = json.load(f)\n","\n","    target_fl_nm = code_path + target_fl_nm\n","    with open(target_fl_nm) as f:\n","        target_data = json.load(f)\n","\n","    return source_data, target_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"35dLAF58pdgT"},"source":["def modfWord(word):\n","    word = word.lower()\n","    wordnet_lemmatizer = WordNetLemmatizer()\n","    word = wordnet_lemmatizer.lemmatize(word)\n","\n","    return word\n","\n","\n","def removeStopWords(word):\n","    stop_word_lst = [\"of\", \"the\", \"system\", \"a\", \"all\", \"at\", \"or\", \"and\", \"to\", \"with\"]\n","    if (word.lower() in stop_word_lst):\n","        return \"\"\n","    else:\n","        return word\n","\n","\n","def getEntityWords(entity):\n","    return_words = []\n","    entity_words = entity.replace(\"'\", \"\").replace('_', ' ').replace('-', ' ').replace('/', ' ').replace('(',\n","                                                                                                         ' ').replace(\n","        ')', ' ').split()\n","    wordnet_lemmatizer = WordNetLemmatizer()\n","    for idx, w in enumerate(entity_words):\n","        word = entity_words[idx]\n","        word = word.lower()\n","        word = wordnet_lemmatizer.lemmatize(word)\n","        word = removeStopWords(word)\n","        if (word != \"\"):\n","            return_words.append(word)\n","\n","    return return_words\n","\n","\n","def checkIfRomanNumeral(intVal):\n","    intVal_Tmp = intVal.upper()\n","    validRomanNumerals = [\"M\", \"D\", \"C\", \"L\", \"X\", \"V\", \"I\"]\n","    for letters in intVal_Tmp:\n","        if letters not in validRomanNumerals:\n","            return False\n","\n","    return True\n","\n","\n","def chkAlphaNumeric(input):\n","    return bool(re.match('^(?=.*[0-9]$)(?=.*[a-zA-Z])', input))\n","\n","\n","def int_to_roman(input):\n","    intVal_Tmp = input\n","    intVal_Tmp = intVal_Tmp.upper()\n","    nums = {'M': 1000, 'D': 500, 'C': 100, 'L': 50, 'X': 10, 'V': 5, 'I': 1}\n","    sum = 0\n","    for i in range(len(intVal_Tmp)):\n","        try:\n","            value = nums[intVal_Tmp[i]]\n","            # If the next place holds a larger number, this value is negative\n","            if (i + 1 < len(intVal_Tmp)) and (nums[intVal_Tmp[i + 1]] > value):\n","                sum -= value\n","            else:\n","                sum += value\n","        except KeyError:\n","            raise (ValueError, 'input is not a valid Roman numeral: %s' % intVal_Tmp)\n","\n","    return sum\n","\n","\n","def crtAltLbl(conf, data, removed_fl):\n","    err_key = []\n","\n","    for key in data.keys():\n","        alt_word = \"\"\n","\n","        if (data[key]['lbl'] is not None):\n","            words = getEntityWords(data[key]['lbl'])  # get all the words separated\n","            for word in words:\n","                tmp_word = word\n","\n","                # if the word is numeric\n","                if (tmp_word.isdigit()):\n","                    num = str(int(tmp_word))\n","                    alt_word = alt_word + \" \" + num\n","                    continue\n","\n","                # if the word is alpha-numeric\n","                if (chkAlphaNumeric(tmp_word)):\n","                    num = \"\".join(re.findall('\\d+', tmp_word))\n","\n","                    if (len(num) > 0):\n","                        num = str(int(num))\n","                        alt_word = alt_word + \" \" + num\n","\n","                    abbr = tmp_word.replace(num, \"\").lower()\n","                    if (abbr == \"s\"):\n","                        alt_word = alt_word + \" \" + modfWord(\"Sacral\")\n","                    elif (abbr == \"l\"):\n","                        alt_word = alt_word + \" \" + modfWord(\"Lumbar\")\n","                    elif (abbr == \"t\"):\n","                        alt_word = alt_word + \" \" + modfWord(\"Thoracic\")\n","                    elif (abbr == \"c\"):\n","                        alt_word = alt_word + \" \" + modfWord(\"Cervical\")\n","                    elif (abbr == \"ca\"):\n","                        alt_word = alt_word + \" \" + modfWord(\"Cornu\") + \" \" + modfWord(\"Ammonis\")\n","                    else:  # CD4,CD8\n","                        alt_word = alt_word + \" \" + modfWord(abbr)\n","                    continue\n","\n","                # if the word is Roman Integer (alpha)\n","                if (tmp_word.isalpha() and checkIfRomanNumeral(tmp_word)):\n","                    roman_num = str(int_to_roman(tmp_word))\n","                    if (roman_num.isdigit()):\n","                        alt_word = alt_word + \" \" + roman_num\n","                    continue\n","\n","                # if the word is simple Literal alpha)\n","                if (tmp_word.isalpha() and (len(tmp_word) >= 1)):\n","                    alt_word = alt_word + \" \" + tmp_word\n","                else:\n","                    removed_fl.write(tmp_word + '\\n')  # these words are removed while modifying labels\n","\n","        else:\n","            print(\"label is None\" + key)\n","            err_key.append(key)\n","\n","        alt_word = ' '.join(sorted(set(alt_word.split())))  # to remove repeat words\n","        data[key]['altLbl'] = alt_word.strip()\n","\n","    for key in err_key:\n","        data.pop(key, None)\n","        removed_fl.write(key + ' :label is None \\n')  # these labels are none\n","\n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SK_QAQYFg3lv"},"source":["def crtAltLblUtl(conf, source_data, target_data, data_src_nm):\n","  \n","    removed_fl = open(conf['removed_utl_fl_nm'], \"w\")\n","    source_data = crtAltLbl(conf, source_data, removed_fl)\n","    target_data = crtAltLbl(conf, target_data, removed_fl)\n","    removed_fl.close()\n","\n","    return source_data, target_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ivC2VRFXhAOW"},"source":["def saveSrcTrgt(source_data, target_data, conf):\n","    with open(code_path + conf['src_op_fl_nm'], 'w') as outfile:\n","        json.dump(source_data, outfile, indent=4)\n","\n","    with open(code_path + conf['trgt_op_fl_nm'], 'w') as outfile:\n","        json.dump(target_data, outfile, indent=4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7AXEPlIjf4OG"},"source":["def modifyLblMain(data_param):\n","    print(\"#################### ModifyLabel START ####################\")\n","    try:\n","        conf_arr = assignVar()\n","        for conf in conf_arr:\n","            source_data, target_data = loadSourceTarget(conf)\n","\n","            source_data, target_data = crtAltLblUtl(conf, source_data, target_data, data_param['db_nm'])\n","\n","            saveSrcTrgt(source_data, target_data, conf)\n","\n","            time.sleep(wait_time)\n","\n","    except Exception as exp:\n","        raise exp\n","    finally:\n","        print(\"#################### ModifyLabel FINISH ####################\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rj7WbuqaqP2q","executionInfo":{"status":"ok","timestamp":1614188383400,"user_tz":420,"elapsed":19378,"user":{"displayName":"Jaydeep Chakraborty","photoUrl":"","userId":"15937785283944377251"}},"outputId":"945b722c-9959-42db-d760-33a5bb24bb4f"},"source":["### MAIN FUNCTION\n","if __name__==\"__main__\":\n","  data_param = getDataParam()\n","  modifyLblMain(data_param['db'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["#################### ModifyLabel START ####################\n","#################### ModifyLabel FINISH ####################\n"],"name":"stdout"}]}]}